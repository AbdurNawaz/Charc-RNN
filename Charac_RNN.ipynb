{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Charac-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heby-MvkeAB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3UJKHrbeBQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('anna.txt') as f:\n",
        "    text = f.read()\n",
        "vocab = sorted(set(text))\n",
        "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UAlKq8OeSd-",
        "colab_type": "code",
        "outputId": "b852c5de-bc97-46a6-d65f-c78d55edc5b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6sVGPNBeVfK",
        "colab_type": "code",
        "outputId": "13aca7bf-998c-4caa-dd34-85306a73e3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
              "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
              "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
              "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
              "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
              "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmDRe1xgeXp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, n_steps):\n",
        "  char_per_batch = batch_size*n_steps\n",
        "  num_batches = len(arr)//char_per_batch\n",
        "\n",
        "  arr = arr[:num_batches*char_per_batch]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  for i in range(0, arr.shape[1], n_steps):\n",
        "    x = arr[:, i:i+n_steps]\n",
        "    y_temp = arr[:, i+1:i+n_steps+1]\n",
        "\n",
        "    y = np.zeros(x.shape, dtype=x.dtype)\n",
        "    y[:, :y_temp.shape[1]] = y_temp\n",
        "\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pVGb8iKz5LS",
        "colab_type": "code",
        "outputId": "d4e1714d-58f1-45b6-8fef-e02967843e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "batches = get_batches(encoded, 10, 50)\n",
        "x, y = next(batches)\n",
        "print('x\\n', x[:, :10])\n",
        "print('y\\n', y[:, :10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[31 64 57 72 76 61 74  1 16  0]\n",
            " [ 1 57 69  1 70 71 76  1 63 71]\n",
            " [78 65 70 13  0  0  3 53 61 75]\n",
            " [70  1 60 77 74 65 70 63  1 64]\n",
            " [ 1 65 76  1 65 75 11  1 75 65]\n",
            " [ 1 37 76  1 79 57 75  0 71 70]\n",
            " [64 61 70  1 59 71 69 61  1 62]\n",
            " [26  1 58 77 76  1 70 71 79  1]\n",
            " [76  1 65 75 70  7 76 13  1 48]\n",
            " [ 1 75 57 65 60  1 76 71  1 64]]\n",
            "y\n",
            " [[64 57 72 76 61 74  1 16  0  0]\n",
            " [57 69  1 70 71 76  1 63 71 65]\n",
            " [65 70 13  0  0  3 53 61 75 11]\n",
            " [ 1 60 77 74 65 70 63  1 64 65]\n",
            " [65 76  1 65 75 11  1 75 65 74]\n",
            " [37 76  1 79 57 75  0 71 70 68]\n",
            " [61 70  1 59 71 69 61  1 62 71]\n",
            " [ 1 58 77 76  1 70 71 79  1 75]\n",
            " [ 1 65 75 70  7 76 13  1 48 64]\n",
            " [75 57 65 60  1 76 71  1 64 61]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scZKGbSb0IQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_inputs(batch_size, n_steps):\n",
        "  inputs = tf.placeholder(tf.int32, [batch_size, n_steps], 'inputs')\n",
        "  targets = tf.placeholder(tf.int32, [batch_size, n_steps], 'targets')\n",
        "\n",
        "  keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "  return inputs, targets, keep_prob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl-oELmH3vbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
        "\n",
        "  def build_cell(lstm_size, keep_prob):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "\n",
        "    return drop\n",
        "  \n",
        "  cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
        "  initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "  return cell, initial_state\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhWDgjqe9B0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_output(lstm_output, in_size, out_size):\n",
        "  seq_output = tf.concat(lstm_output, axis=1)\n",
        "  x = tf.reshape(seq_output, [-1, in_size])\n",
        "\n",
        "  with tf.variable_scope('softmax'):\n",
        "    softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
        "    softmax_b = tf.Variable(tf.zeros(out_size))\n",
        "\n",
        "  logits = tf.matmul(x, softmax_w) + softmax_b\n",
        "\n",
        "  out = tf.nn.softmax(logits, name='predictions')\n",
        "\n",
        "  return out, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zZw15w-DlTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_loss(logits, targets, lstm_size, num_classes):\n",
        "\n",
        "  y_one_hot = tf.one_hot(targets, num_classes)\n",
        "  y_reshaped = tf.reshape(y_one_hot, logits.shape)\n",
        "\n",
        "  loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
        "  loss = tf.reduce_mean(loss)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD7y7CcyFf0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clipping Gradients to avoid exploding\n",
        "\n",
        "def build_optimizer(loss, learning_rate, grad_clip):\n",
        "  tvars = tf.trainable_variables()\n",
        "  grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
        "  train_op = tf.train.AdamOptimizer(learning_rate)\n",
        "  optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
        "\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ6STtrMGn5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class charRNN:\n",
        "\n",
        "  def __init__(self, num_classes, batch_size=64, num_steps=50, lstm_size=128,\n",
        "               num_layers=2, learning_rate=0.001, grad_clip=5, sampling=False):\n",
        "    if sampling == True:\n",
        "      batch_size, num_steps = 1, 1\n",
        "    else:\n",
        "      batch_size, num_steps = batch_size, num_steps\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
        "    cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
        "\n",
        "    x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
        "\n",
        "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
        "    self.final_state = state \n",
        "    self.predication, self.logits = build_output(outputs,lstm_size, num_classes)\n",
        "\n",
        "    self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
        "    self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtowiabZJfsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HYPERPARAMETERS\n",
        "\n",
        "batch_size = 100\n",
        "num_steps = 100\n",
        "lstm_size = 512\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "keep_prob = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ei0DdtGr_Jw",
        "colab_type": "code",
        "outputId": "7956b14a-838d-43c5-ff16-0c2d5e314e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "epochs = 20\n",
        "print_every = 50\n",
        "\n",
        "save_every = 200\n",
        "\n",
        "model = charRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
        "                lstm_size=lstm_size, num_layers=num_layers, learning_rate=learning_rate)\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=100)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  c = 0\n",
        "  for  e in range(epochs):\n",
        "    new_state = sess.run(model.initial_state)\n",
        "    loss = 0\n",
        "    for x, y in get_batches(encoded, batch_size, num_steps):\n",
        "      c += 1\n",
        "      start = time.time()\n",
        "      feed = {model.inputs: x,\n",
        "              model.targets: y,\n",
        "              model.keep_prob: keep_prob,\n",
        "              model.initial_state: new_state}\n",
        "      batch_loss, new_state, _ = sess.run([model.loss,\n",
        "                                           model.final_state,\n",
        "                                           model.optimizer], feed_dict=feed)\n",
        "      if c%print_every == 0:\n",
        "        end = time.time()\n",
        "        print('Epoch: {}/{}...'.format(e+1, epochs),\n",
        "              'Trainint Step: {}...'.format(c),\n",
        "               'Training loss: {:.4f}...'.format(batch_loss)\n",
        "               ,'{:.4f} sec/batch'.format(end-start))\n",
        "  saver.save(sess, 'checkpoints/i{}_l{}.ckpt'.format(c, lstm_size))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8d044ff80486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m       batch_loss, new_state, _ = sess.run([model.loss,\n\u001b[1;32m     26\u001b[0m                                            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                            model.optimizer], feed_dict=feed)\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruQa1VaxDQOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "23dba367-0adb-436e-ffa0-af13d313cce4"
      },
      "source": [
        "epochs = 20\n",
        "# Print losses every N interations\n",
        "print_every_n = 50\n",
        "\n",
        "# Save every N iterations\n",
        "save_every_n = 200\n",
        "\n",
        "model = charRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
        "                lstm_size=lstm_size, num_layers=num_layers, \n",
        "                learning_rate=learning_rate)\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=100)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Use the line below to load a checkpoint and resume training\n",
        "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
        "    counter = 0\n",
        "    for e in range(epochs):\n",
        "        # Train network\n",
        "        new_state = sess.run(model.initial_state)\n",
        "        loss = 0\n",
        "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
        "            counter += 1\n",
        "            start = time.time()\n",
        "            feed = {model.inputs: x,\n",
        "                    model.targets: y,\n",
        "                    model.keep_prob: keep_prob,\n",
        "                    model.initial_state: new_state}\n",
        "            batch_loss, new_state, _ = sess.run([model.loss, \n",
        "                                                 model.final_state, \n",
        "                                                 model.optimizer], \n",
        "                                                 feed_dict=feed)\n",
        "            if (counter % print_every_n == 0):\n",
        "                end = time.time()\n",
        "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
        "                      'Training Step: {}... '.format(counter),\n",
        "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
        "                      '{:.4f} sec/batch'.format((end-start)))\n",
        "        \n",
        "            if (counter % save_every_n == 0):\n",
        "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
        "    \n",
        "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1551...  7.3041 sec/batch\n",
            "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0798...  7.3562 sec/batch\n",
            "Epoch: 1/20...  Training Step: 150...  Training loss: 2.7703...  7.5919 sec/batch\n",
            "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4216...  6.9708 sec/batch\n",
            "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3422...  7.5976 sec/batch\n",
            "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2095...  7.1731 sec/batch\n",
            "Epoch: 2/20...  Training Step: 350...  Training loss: 2.1548...  7.5330 sec/batch\n",
            "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0366...  7.1146 sec/batch\n",
            "Epoch: 3/20...  Training Step: 450...  Training loss: 1.9693...  6.9560 sec/batch\n",
            "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9009...  7.2840 sec/batch\n",
            "Epoch: 3/20...  Training Step: 550...  Training loss: 1.8764...  7.3871 sec/batch\n",
            "Epoch: 4/20...  Training Step: 600...  Training loss: 1.7707...  7.8900 sec/batch\n",
            "Epoch: 4/20...  Training Step: 650...  Training loss: 1.7895...  7.5023 sec/batch\n",
            "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7249...  7.0757 sec/batch\n",
            "Epoch: 4/20...  Training Step: 750...  Training loss: 1.6974...  7.2237 sec/batch\n",
            "Epoch: 5/20...  Training Step: 800...  Training loss: 1.6639...  7.0868 sec/batch\n",
            "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6265...  7.3857 sec/batch\n",
            "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6126...  6.9028 sec/batch\n",
            "Epoch: 5/20...  Training Step: 950...  Training loss: 1.5975...  6.9284 sec/batch\n",
            "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.5755...  7.1715 sec/batch\n",
            "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.5829...  6.9887 sec/batch\n",
            "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.5423...  7.2029 sec/batch\n",
            "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.5394...  6.6901 sec/batch\n",
            "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.4939...  6.6004 sec/batch\n",
            "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5512...  6.8671 sec/batch\n",
            "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.4508...  7.7001 sec/batch\n",
            "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.4690...  6.9975 sec/batch\n",
            "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.4560...  7.3201 sec/batch\n",
            "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4342...  6.8336 sec/batch\n",
            "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.3954...  7.1537 sec/batch\n",
            "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.3995...  7.1291 sec/batch\n",
            "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.3535...  7.4673 sec/batch\n",
            "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.3830...  6.8030 sec/batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdlLAvsuL-bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}